{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/afalefesifiofo/rag_system/blob/main/Copia_di_RAG_zephyr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval Augmented Generation (RAG) Pipelines with Haystack\n",
        "\n",
        "A Retrieval Augmented Generation (RAG) system is developed within this notebook using Haystack. This system integrates two different components:\n",
        "* **indexing pipeline**: in which context data is processed, vectorized, and saved to a local vector DB.\n",
        "* **retrieval pipeline**: through which context data can be retrieved based on a query performed by the user.\n",
        "\n",
        "## Summary\n",
        "[Retrieval Augmented Generation (RAG) Pipelines with Haystack 2.0](#scrollTo=ds09UVrMiI38)\n",
        "\n",
        "> [Introduction to RAG systems](#scrollTo=VlRLgGMQuYRr)\n",
        "\n",
        "> [Install the project dependencies](#scrollTo=C_EmS8KTLqaG)\n",
        "\n",
        "> [Import the project libraries](#scrollTo=cxaAoBRwVZCk)\n",
        "\n",
        "> [Download the Dataset](#scrollTo=HWu0v6ObdpAW)\n",
        "\n",
        "> [Load the Dataset](#scrollTo=xwwt1A8KVJlX)\n",
        "\n",
        "> [Create the Documents to indexing](#scrollTo=gX_dditMhrvh)\n",
        "\n",
        "> [The indexing Pipeline](#scrollTo=_Gqn28JJtYXn)\n",
        "\n",
        "> [The RAG Pipeline](#scrollTo=bxLazyNtrHJ4)\n",
        "\n",
        "> [Create the RAG Pipeline](#scrollTo=uUHdCn1KWw6A)\n",
        "\n",
        "> [Test the RAG Pipeline](#scrollTo=AyMknm7PgN2M)\n",
        "\n",
        "\n",
        "*Notebook created by [Andrea Cadeddu](https://it.linkedin.com/in/andrea-cadeddu) and [Vincenzo De Leo](https://it.linkedin.com/in/vincenzodeleo) from [Linkalab](https://www.linkalab.it/).*"
      ],
      "metadata": {
        "id": "ds09UVrMiI38"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to RAG systems\n",
        "\n",
        "Retrieval-Augmented Generation (RAG) is the process of optimizing the output of a large language model so that it references an authoritative knowledge base outside its training data sources before generating an answer. RAG extends the already advanced capabilities of LLMs to specific domains or to an organization's internal knowledge base, all without the need to retrain the model. It is a cost-effective approach to improving LLM output so that it remains relevant, accurate, and useful in various contexts.  \n",
        "\n",
        "RAG technology offers several benefits to an organization's generative AI efforts:\n",
        "* efficient implementation\n",
        "* easy data management\n",
        "* increased user confidence\n",
        "* increased control by developers\n",
        "\n",
        "### How does Retrieval-Augmented Generation work?\n",
        "Without RAG, the LLM takes user input and creates a response based on the information it has been trained on or what it already knows. With RAG, an information retrieval component is introduced that uses user input to first extract information from a new data source. The user's request and the relevant information are both provided to the LLM. The LLM uses the new knowledge and its training data to create better answers. The following sections provide an overview of the process.\n",
        "\n",
        "<center> Example of RAG flow </center>\n",
        "<img src=https://haystack.deepset.ai/blog/mixtral-8x7b-healthcare-chatbot/RAG.png>\n",
        "\n",
        "**Creation of external data**  \n",
        "New data outside the LLM's original training dataset is called external data. It can come from multiple data sources, such as APIs, databases, or document repositories. The data can exist in various formats such as files, database records or long text. Another artificial intelligence technique, called language model embedding, converts the data into numerical representations and stores them in a vector database. This process creates a library of knowledge that generative AI models can understand.\n",
        "\n",
        "**Retrieval of relevant information**  \n",
        "The next step is to perform a relevance query. The user's query is converted into a vector representation and matched with vector databases. The documents retrieved will be returned because they are highly relevant to the user's question. Relevance was calculated and established using vector mathematical calculations and representations.\n",
        "\n",
        "**Increasing the LLM prompt**  \n",
        "Next, the RAG model augments the user input (or prompts) by adding relevant retrieved data in context. This step uses prompt design techniques to communicate effectively with the LLM. The augmented prompt allows large language models to generate an accurate response to user queries.\n",
        "\n",
        "**Updating external data**\n",
        "The next question might be: what happens if the external data becomes obsolete?To maintain current information for retrieval, asynchronously update the documents and update the embedded representation of the documents.You can do this through automated real-time processes or periodic batch processing.This is a common challenge in data analysis: you can use different data science approaches to change management.\n",
        "\n",
        "**Sources**\n",
        "* https://aws.amazon.com/it/what-is/retrieval-augmented-generation/\n",
        "* https://haystack.deepset.ai/blog/mixtral-8x7b-healthcare-chatbot"
      ],
      "metadata": {
        "id": "VlRLgGMQuYRr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install the project dependencies  \n",
        "In order to implement the RAG pipeline, the following libraries must be installed:\n",
        "* [accelerate](https://huggingface.co/docs/accelerate/index) and [bitsandbytes](https://huggingface.co/docs/bitsandbytes/main/en/index): required to use quantized versions of LLM models (with smaller memory footprint)\n",
        "* [haystack-ai](https://haystack.deepset.ai/): necessary to create the indexing and RAG pipelines\n",
        "* [pandas](https://pandas.pydata.org/): enables easy management of tabular data\n",
        "* [rich](https://rich.readthedocs.io/en/stable/introduction.html): allows to get better display of terminal outputs\n",
        "* [seaborn](https://pandas.pydata.org/): for better data visualization\n",
        "* [sentence-transformers](https://www.sbert.net/): to convert text data into embeddings\n",
        "\n",
        "**Please Note**  \n",
        "The magic command `capture` hides the package installation output"
      ],
      "metadata": {
        "id": "C_EmS8KTLqaG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyvRJLUrKecZ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install accelerate bitsandbytes haystack-ai pandas rich seaborn sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import the project libraries\n",
        "As can be seen from the list of imported libraries, Haystack organizes all components by functionality.  \n",
        "For details on source code organization, please refer to the [repository](https://github.com/deepset-ai/haystack) on GitHub."
      ],
      "metadata": {
        "id": "cxaAoBRwVZCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "from haystack import Document\n",
        "from haystack import Pipeline\n",
        "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
        "from haystack.document_stores.types import DuplicatePolicy\n",
        "from haystack.components.preprocessors import DocumentCleaner\n",
        "from haystack.components.preprocessors import DocumentSplitter\n",
        "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
        "from haystack.components.writers import DocumentWriter\n",
        "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
        "from haystack.components.retrievers.in_memory import InMemoryBM25Retriever\n",
        "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
        "from haystack.components.joiners import DocumentJoiner\n",
        "from haystack.components.rankers import TransformersSimilarityRanker\n",
        "from haystack.components.builders import PromptBuilder\n",
        "from haystack.components.generators import HuggingFaceLocalGenerator\n",
        "from haystack.utils import ComponentDevice\n",
        "\n",
        "import rich\n",
        "\n",
        "import hashlib\n",
        "import warnings\n",
        "warnings.filterwarnings(action=\"ignore\")"
      ],
      "metadata": {
        "id": "s3REI7DlVYLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define libraries configurations\n",
        "New default parameters are defined to aid data visualization"
      ],
      "metadata": {
        "id": "qGdhwKdk62rI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'"
      ],
      "metadata": {
        "id": "2srxbJRCxmvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option(\"display.float_format\", '{:.2f}'.format)"
      ],
      "metadata": {
        "id": "SWq152gyZFvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Donwload the Dataset\n",
        "Through the bash function `download_data_from_google_drive`, it is possible to download a shared file uploaded to Google Drive."
      ],
      "metadata": {
        "id": "HWu0v6ObdpAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "download_data_from_google_drive () {\n",
        "    html=`curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=${1}\"`\n",
        "    curl -Lb ./cookie \"https://drive.google.com/uc?export=download&`echo ${html}|grep -Po '(confirm=[a-zA-Z0-9\\-_]+)'`&id=${1}\" -o ${2}\n",
        "    echo \"${2} downloaded\"\n",
        "    rm ./cookie\n",
        "}\n",
        "\n",
        "mkdir data\n",
        "download_data_from_google_drive \"1C2svQCaT7nlD9O08gmQ6Di3jc6vGh8EI\" \"data/imdb_movies.csv\""
      ],
      "metadata": {
        "id": "uvz7ZxBaU1lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the Dataset\n",
        "The dataset is extracted from [IMDb](https://www.imdb.com/) and contains the records of several movies released in recent years.  \n",
        "Specifically, this is the data dictionary of the data:\n",
        "* <u>name</u> [string]: name of the movie\n",
        "* <u>date</u> [date]: release date of the movie\n",
        "* <u>score</u> [float]: average critics rating\n",
        "* <u>genre</u> [str]: movie genres\n",
        "* <u>description</u> [str]: description of the movie\n",
        "* <u>cast</u> [str]: list with cast members\n",
        "\n"
      ],
      "metadata": {
        "id": "0hDjelWbwtmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "usecols = [\"names\", \"date_x\", \"score\", \"genre\", \"overview\", \"crew\"]\n",
        "\n",
        "read_csv_config = {\n",
        "    \"filepath_or_buffer\": \"data/imdb_movies.csv\",\n",
        "    \"usecols\": usecols,\n",
        "    \"parse_dates\": [\"date_x\"]\n",
        "}\n",
        "rename_columns = {\n",
        "    \"names\": \"name\",\n",
        "    \"date_x\": \"date\",\n",
        "    \"overview\": \"description\",\n",
        "    \"crew\": \"cast\",\n",
        "}\n",
        "\n",
        "df = pd.read_csv(**read_csv_config) \\\n",
        "       .rename(columns=rename_columns)\n",
        "df[\"date\"] = df[\"date\"].dt.year\n",
        "df"
      ],
      "metadata": {
        "id": "xwwt1A8KVJlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Show movies score distribution\n",
        "To get a better understanding of the data, the distribution of ratings provided by critics is analyzed.\n",
        "As can be seen from the graph, the distribution of evaluations tends toward a normal distribution, with an evident level of (negative) skewness.  \n",
        "The level of skewness highlighted seems to be caused by the presence of several moveis with a very low rating and tending toward zero."
      ],
      "metadata": {
        "id": "eaoAKna59ngM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.title(label=\"Score distribution\")\n",
        "sns.histplot(data=df, x=\"score\", kde=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ov74041m9SOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyze documents length distribution\n",
        "As can be guessed from the <u>description</u> of the columns in the dataset, the movie description turns out to be very informative and potentially useful for recommending movies to the user based on his or her preferences.  \n",
        "Therefore, a quick analysis is performed to calculate and visualize the length of the movie descriptions.  \n",
        "As can be seen from the histograms, movie descriptions tend to be short and concise."
      ],
      "metadata": {
        "id": "SFdfoqJ6wvgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the description lenghts\n",
        "document_chars = df[\"description\"].str.len()\n",
        "document_words = df[\"description\"].str.split().str.len()\n",
        "document_sents = (df[\"description\"].str.count(\"\\. \") + 1)\n",
        "\n",
        "# Perform the visualization of description lenghts\n",
        "fig, ax = plt.subplots(figsize=(20, 5), nrows=1, ncols=3)\n",
        "fig.suptitle(t=\"Documents length distribution\")\n",
        "sns.histplot(data=document_chars, kde=True, ax=ax[0])\n",
        "sns.histplot(data=document_words, kde=True, ax=ax[1])\n",
        "sns.countplot(x=document_sents, ax=ax[2])\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "2d1I5UArvaPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filter movies by score\n",
        "Since we don't want to recommend bad movies, the movies are filtered using the score value.\n"
      ],
      "metadata": {
        "id": "ZonKxEs1eMO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_documents = df.query(expr=\"score >= 60\")\n",
        "df_documents"
      ],
      "metadata": {
        "id": "iQzpvm27ecl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the Documents to indexing\n",
        "In order to process and upload data to Haystack, it is first necessary to encapsulate everything within a `Document`.  \n",
        "Therefore, an iteration is performed to create a `Document` for each movie in the Dataset.  \n",
        "Specifically, the content of the <u>description</u> is loaded as content, while the variables <u>name</u>, <u>date</u>, <u>genre</u>, and <u>cast</u> are considered metadata."
      ],
      "metadata": {
        "id": "907u3PtIhlWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = []\n",
        "\n",
        "for index, data in df_documents.iterrows():\n",
        "  meta = {\n",
        "      \"name\": data[\"name\"],\n",
        "      \"year\": data[\"date\"],\n",
        "      \"genre\": data[\"genre\"],\n",
        "      \"cast\": data[\"cast\"],\n",
        "      \"score\": int(data[\"score\"])\n",
        "  }\n",
        "  document = Document(content=data[\"description\"], meta=meta)\n",
        "  documents.append(document)\n",
        "\n",
        "# show the first 5 documents\n",
        "rich.print(documents[:5])"
      ],
      "metadata": {
        "id": "gX_dditMhrvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How hash works\n",
        "The module `hashlib` implements a common interface to many different secure hash and message digest algorithms."
      ],
      "metadata": {
        "id": "kM5iTSdZDaR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m = hashlib.sha256()\n",
        "m.update(b\"After dominating the boxing world, Adonis Creed has been thriving in both his career and family life...\")\n",
        "m.hexdigest()"
      ],
      "metadata": {
        "id": "vW88wntEDZ1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The indexing Pipeline\n",
        "The indexing Pipeline transform the original Documents and save them in the Document Store.\n",
        "\n",
        "It consists of several components:\n",
        "1. `DocumentCleaner`: performs a basic cleaning of the Documents\n",
        "2. `DocumentSplitter`: chunks each `Document` into smaller pieces (more appropriate for semantic search and RAG)\n",
        "3. `SentenceTransformersDocumentEmbedder`:\n",
        "  * represent each `Document` as a vector (capturing its meaning) using a specified model\n",
        "  * also the metadata is embedded, because it contains relevant information (`metadata_fields_to_embed` parameter).\n",
        "  * the GPU is used for this expensive operation (`device` parameter).\n",
        "4. `DocumentWriter` just saves the Documents in the Document Store"
      ],
      "metadata": {
        "id": "_Gqn28JJtYXn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the Document Store\n",
        "A vector database, unlike traditional databases, is a specialized database designed to store, manage and search embedded vectors. It stores numerical representations of documents. Breaking down data to numerical embeddings makes it easier for our AI system to understand and process the data.\n",
        "\n",
        "* `InMemoryDocumentStore` is used to store movie data with its embeddings, which stores the data in the device RAM memory.\n",
        "\n",
        "However, it should be specified that in case the database should have a considerable size (not this case), it is necessary to use ad hoc [vector database](https://haystack.deepset.ai/integrations?type=Document+Store&maintainer=deepset) services.\n",
        "\n",
        "<center>Overview of vector databases services</center>\n",
        "<center>\n",
        "  <img src=https://images.datacamp.com/image/upload/v1714147215/image_c9031ee72f.png width=800>\n",
        "</center>\n",
        "\n",
        "Each vector database has different characteristics and strengths, so choosing the ideal service depends on your needs and the data available.\n",
        "\n"
      ],
      "metadata": {
        "id": "ezgwA7uCXbr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document_store = InMemoryDocumentStore(\n",
        "    # the regex search for all words of at least two characters, considering also non-ASCII characters\n",
        "    bm25_tokenization_regex=\"(?u)\\\\b\\\\w\\\\w+\\\\b\",\n",
        "    bm25_algorithm=\"BM25L\",\n",
        "    bm25_parameters=None,\n",
        "    embedding_similarity_function=\"cosine\"\n",
        ")"
      ],
      "metadata": {
        "id": "i_jg7dkuaK0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the components for text processing\n",
        "\n",
        "* `DocumentCleaner` makes text documents more readable. It removes extra whitespaces, empty lines, specified substrings, regexes, page headers, and footers in this particular order. This is useful for preparing the documents for further processing by LLMs.\n",
        "\n",
        "* `DocumentSplitter` divides a list of text documents into a list of shorter text Documents. This is useful for long texts that otherwise wouldn't fit into the maximum text length of language models and can also speed up question answering.\n",
        "\n",
        "The `DocumentSplitter` takes care of splitting texts into chunks when they exceed a certain length. This processing step is critical to properly preserving and vectorizing all the information in the database.\n",
        "\n",
        "\n",
        "<center>Overview of the `DocumentSplitter` flow</center>\n",
        "<center>\n",
        "  <img src=https://miro.medium.com/v2/resize:fit:1100/format:webp/0*v-rMAW9xuKERbyJF.png width=600>\n",
        "</center>\n",
        "\n",
        "In addition, making a split of texts into chunks allows for better prompt optimization, as the extracted documents will be shorter and there will be less chance of eroding the context window of the model."
      ],
      "metadata": {
        "id": "OMVcpZL4quFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document_cleaner = DocumentCleaner(\n",
        "    remove_empty_lines=True,\n",
        "    remove_extra_whitespaces=True,\n",
        "    remove_repeated_substrings=False,\n",
        "    remove_substrings=None,\n",
        "    remove_regex=None\n",
        ")\n",
        "\n",
        "document_splitter = DocumentSplitter(\n",
        "    split_by='word',\n",
        "    split_length=256,\n",
        "    split_overlap=0\n",
        ")"
      ],
      "metadata": {
        "id": "ALy9U8ICqtjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the components for text vectorization\n",
        "Before we store our data, we will need to convert it to vector embeddings before it is stored in the database.  \n",
        "Haystack provides a connector so that models in this family can be used directly by specifying the model name.  \n",
        "The model used is [bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5) provided by the Beijing research team, which turns out to be very accurate in the calculation of embeddings.\n",
        "\n",
        "**Note**  \n",
        "Metadata embeddings can also be computed, whereby the list with the metadata to be included in the vectorization operation is defined."
      ],
      "metadata": {
        "id": "-b5q6s6N-ITE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "\n",
        "document_embedder = SentenceTransformersDocumentEmbedder(\n",
        "    model=\"BAAI/bge-large-en-v1.5\",\n",
        "    device=ComponentDevice.from_str(\"cuda:0\"),\n",
        "    # token=None,\n",
        "    prefix='',\n",
        "    suffix='',\n",
        "    batch_size=batch_size,\n",
        "    progress_bar=True,\n",
        "    normalize_embeddings=False,\n",
        "    meta_fields_to_embed=[\"name\", \"year\", \"genre\", \"cast\"],\n",
        "    embedding_separator='\\n'\n",
        ")\n",
        "document_embedder.warm_up()"
      ],
      "metadata": {
        "id": "KiVAb6bWrl5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Try the embedding model"
      ],
      "metadata": {
        "id": "fMNMYdrMEhP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_embeddings = document_embedder.run(documents=[Document(\"The cat is on the table\")])\n",
        "test_embeddings"
      ],
      "metadata": {
        "id": "DTPeYRQQElLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the document writer\n",
        "`DocumentWriter` writes a list of Documents into a Document Store. It's typically used in an indexing pipeline as the final step after preprocessing Documents and creating their embeddings."
      ],
      "metadata": {
        "id": "SWFs9O1Tq4RW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document_writer = DocumentWriter(\n",
        "    document_store=document_store,\n",
        "    # if duplicates are identified during the writing phase, they are overwritten\n",
        "    policy=DuplicatePolicy.OVERWRITE\n",
        ")"
      ],
      "metadata": {
        "id": "WxrL38B-q4YR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the indexing Pipeline\n",
        "The pipelines in Haystack are directed multigraphs of different Haystack components and integrations. They give the freedom to connect these components in various ways. This means that the pipeline doesn't need to be a continuous stream of information.\n",
        "\n",
        "\n",
        "### Steps to Create a Pipeline Explained\n",
        "Once all components are created and ready to be combined in a Pipeline, there are four steps to make it work:\n",
        "\n",
        "1. **Create the Pipeline with Pipeline**: creates the Pipeline object\n",
        "2. **Add components to the Pipeline**: adds components to the Pipeline, without connecting them yet. It's especially useful for loops as it allows to smoothly connect the components in the next step because they all already exist in the Pipeline\n",
        "3. **Connect the Pipeline components**: at this step, you explicitly connect one of the outputs of a component to one of the inputs of the next component. This is also when the Pipeline validates the connection without running the components. It makes the validation fast\n",
        "4. **Run the Pipeline**: finally, you run the Pipeline by specifying the first component in the Pipeline and passing its mandatory inputs"
      ],
      "metadata": {
        "id": "iKboVU8p-DjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add components to your pipeline\n",
        "indexing_pipeline = Pipeline()\n",
        "indexing_pipeline.add_component(name=\"cleaner\", instance=document_cleaner)\n",
        "indexing_pipeline.add_component(name=\"splitter\", instance=document_splitter)\n",
        "indexing_pipeline.add_component(name=\"embedder\", instance=document_embedder)\n",
        "indexing_pipeline.add_component(name=\"writer\", instance=document_writer)\n",
        "\n",
        "# Now, connect the components to each other\n",
        "indexing_pipeline.connect(sender=\"cleaner\", receiver=\"splitter\")\n",
        "indexing_pipeline.connect(sender=\"splitter\", receiver=\"embedder\")\n",
        "indexing_pipeline.connect(sender=\"embedder\", receiver=\"writer\")"
      ],
      "metadata": {
        "id": "B9tmM_jZ-CGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize the indexing Pipeline\n",
        "Is possible to visualize the created pipelines as graphs to better understand how the components are connected."
      ],
      "metadata": {
        "id": "mXnrKAKeiT9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "indexing_pipeline.draw(path=\"indexing_pipeline.png\")\n",
        "Image(\"indexing_pipeline.png\")"
      ],
      "metadata": {
        "id": "Db8soKPWh-Kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Serialize the indexing Pipeline\n",
        "For production systems is possible to serialize the created Pipeline as YAML configuration."
      ],
      "metadata": {
        "id": "Ihiw1OhbtI3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yaml_pipeline = indexing_pipeline.dumps()\n",
        "rich.print(yaml_pipeline)"
      ],
      "metadata": {
        "id": "afmURmNLtJHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Compute the number of batches for the indexing Pipeline\n",
        "The value may be slightly different due to rounding error."
      ],
      "metadata": {
        "id": "eXJ0wWeyvKoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batches = df_documents.shape[0] // batch_size\n",
        "batches"
      ],
      "metadata": {
        "id": "mPu8cfoevFkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run the indexing Pipeline\n",
        "With the `run` method is possible to execute the created Pipeline on the input data."
      ],
      "metadata": {
        "id": "HkpASBmerBS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "indexing_pipeline.run(\n",
        "    data={\"cleaner\": {\"documents\": documents}},\n",
        "    debug=False\n",
        ")"
      ],
      "metadata": {
        "id": "5GoISke1b6BY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The RAG Pipeline\n",
        "The RAG Pipeline finds Documents relevant to the user query and pass them to the LLM to generate a grounded answer.  \n",
        "Here we chose to develop a hybrid Pipeline supported by a Re-Ranker model in order to show a sophisticated solution for building RAG systems.\n",
        "\n",
        "\n",
        "<center>Flow of the RAG Pipeline</center>\n",
        "<img src=https://haystack.deepset.ai/blog/hybrid-retrieval/reranker.png>"
      ],
      "metadata": {
        "id": "bxLazyNtrHJ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the text vectorization Pipeline\n",
        "Retrieval happens when the system tries to quickly find the documents from the index that satisfy the search criteria. The goal of the retriever is to get documents that will be used to provide context and ground the LLM on your data.\n",
        "\n",
        "There are several ways to perform search within our database such as:\n",
        "\n",
        "* **keyword search**: used for text searches using a modified version of tf-idf called [BM25](https://github.com/dorianbrown/rank_bm25)\n",
        "\n",
        "* **semantic search**: uses the semantic meaning of words\n",
        "\n",
        "* **vector search**: converts documents from text to vector representations using embedding models. Retrieval will be done by querying the documents whose vector representations are closest to the user question.\n",
        "\n",
        "* **hybrid**: refers to the combination of multiple retrieval methods to enhance overall performance. In the context of search systems, a hybrid retrieval pipeline executes both traditional keyword-based search and dense vector search, later ranking the results with a cross-encoder model. This combination allows the search system to leverage the strengths of different approaches, providing more accurate and diverse results.\n",
        "\n",
        "**Sources**\n",
        "* https://haystack.deepset.ai/blog/hybrid-retrieval"
      ],
      "metadata": {
        "id": "yEGfbpSiYCBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_embedder = SentenceTransformersTextEmbedder(\n",
        "    model=\"BAAI/bge-large-en-v1.5\",\n",
        "    device=ComponentDevice.from_str(\"cuda:0\"),\n",
        "    # token=None,\n",
        "    prefix='',\n",
        "    suffix='',\n",
        "    batch_size=32,\n",
        "    progress_bar=True,\n",
        "    normalize_embeddings=False\n",
        ")\n",
        "\n",
        "bm25_retriever = InMemoryBM25Retriever(\n",
        "    document_store=document_store,\n",
        "    filters=None,\n",
        "    top_k=10,\n",
        "    scale_score=False\n",
        ")\n",
        "\n",
        "embedding_retriever = InMemoryEmbeddingRetriever(\n",
        "    document_store=document_store,\n",
        "    filters=None,\n",
        "    top_k=10,\n",
        "    scale_score=False,\n",
        "    return_embedding=False\n",
        ")\n",
        "\n",
        "document_joiner = DocumentJoiner(\n",
        "    join_mode='concatenate',\n",
        "    weights=None,\n",
        "    top_k=None,\n",
        "    sort_by_score=True\n",
        ")"
      ],
      "metadata": {
        "id": "N6BK0pku0aAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the Ranker model\n",
        "`TransformersSimilarityRanker` ranks Documents based on how similar they are to the query. It uses a pre-trained cross-encoder model from the Hugging Face Hub to embed both the query and the Documents. It then compares the embeddings to determine how similar they are. The result is a list of Document objects in ranked order, with the Documents most similar to the query appearing first.  \n",
        "\n",
        "`TransformersSimilarityRanker` is most useful in query pipelines, such as a retrieval-augmented generation (RAG) pipeline or a document search pipeline, to ensure the retrieved Documents are ordered by relevance. When using it with a `Retriever`, consider setting the Retriever's `top_k` to a small number. This way the `Ranker` will have fewer Documents to process which can help make your pipeline faster."
      ],
      "metadata": {
        "id": "PXboJjfaYMLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ranker = TransformersSimilarityRanker(\n",
        "    model='BAAI/bge-reranker-base',\n",
        "    device=None,\n",
        "    # token=None,\n",
        "    top_k=5,\n",
        "    query_prefix='',\n",
        "    document_prefix='',\n",
        "    meta_fields_to_embed=None,\n",
        "    embedding_separator='\\n',\n",
        "    scale_score=True,\n",
        "    calibration_factor=1.0,\n",
        "    score_threshold=None,\n",
        "    model_kwargs=None\n",
        ")\n",
        "ranker.warm_up()"
      ],
      "metadata": {
        "id": "7UtDMgeLrcUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the PromptBuilder\n",
        "Use this component in pipelines before a Generator to render a prompt template and fill in variable values.\n",
        "The template that is provided to the `PromptBuilder` during initialization needs to conform to the Jinja2 template language.\n",
        "\n",
        " Let's setup the prompt builder, with a format like the following (appropriate for Zephyr):\n",
        "\n",
        " ```python\n",
        " \"<|system|>\\nSYSTEM MESSAGE</s>\\n<|user|>\\nUSER MESSAGE</s>\\n<|assistant|>\\n\"\n",
        " ```\n",
        "\n",
        "**Sources**  \n",
        "Useful resource for prompting: [Hugging Face - LLM prompting guide](https://huggingface.co/docs/transformers/main/tasks/prompting)"
      ],
      "metadata": {
        "id": "UyLWJ6Z5YO-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"<|system|>Using the information contained in the context, give a comprehensive answer to the question.\n",
        "If the answer cannot be deduced from the context, do not give an answer.</s>\n",
        "<|user|>\n",
        "Context:\n",
        "  {% for doc in documents %}\n",
        "  name: {{ doc.meta['name'] }} content: {{ doc.content }}\n",
        "  {% endfor %};\n",
        "Question: {{query}}\n",
        "</s>\n",
        "<|assistant|>\n",
        "\"\"\"\n",
        "\n",
        "prompt_builder = PromptBuilder(template=prompt_template)"
      ],
      "metadata": {
        "id": "uhmsDUbdS3PR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the Large Language Model (LLM) generator\n",
        "To load and manage Open Source LLMs in Haystack, is possible to use the `HuggingFaceLocalGenerator` that loads the model from Huggin Face Hub.\n",
        "\n",
        "The chosen LLM is [Zephyr 7B Beta](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta), a fine-tuned version of [Mistral 7B V.01](https://huggingface.co/mistralai/Mistral-7B-v0.1) that focuses on helpfulness and outperforms many larger models on the MT-Bench and AlpacaEval benchmarks; the model was fine-tuned by the Hugging Face team.\n",
        "\n",
        "Since we are using a free Colab instance (with limited resources), the model is loaded using 4-bit quantization (passing the appropriate huggingface_pipeline_kwargs to our Generator)."
      ],
      "metadata": {
        "id": "WsTyHgRjV1Qd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "huggingface_pipeline = {\n",
        "    \"device_map\": \"auto\",\n",
        "    \"model_kwargs\":{\n",
        "        \"load_in_4bit\": True,\n",
        "        \"bnb_4bit_use_double_quant\": True,\n",
        "        \"bnb_4bit_quant_type\": \"nf4\",\n",
        "        \"bnb_4bit_compute_dtype\": torch.bfloat16\n",
        "        }\n",
        "}\n",
        "\n",
        "llm = HuggingFaceLocalGenerator(\n",
        "    model=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "    task=\"text-generation\",\n",
        "    # device=ComponentDevice.from_str(\"cuda:0\"),\n",
        "    # token=None,\n",
        "    generation_kwargs={\"max_new_tokens\": 386},\n",
        "    huggingface_pipeline_kwargs=huggingface_pipeline,\n",
        "    stop_words=None\n",
        ")\n",
        "llm.warm_up()"
      ],
      "metadata": {
        "id": "wERJuTTMTLt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Try the LLM"
      ],
      "metadata": {
        "id": "wlJ-68OoBDmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm.run(prompt=\"Scrivimi una descrizione dell'Italia.\")\n",
        "rich.print(response)"
      ],
      "metadata": {
        "id": "zSYevjdNBHGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the Retrieval Pipeline"
      ],
      "metadata": {
        "id": "uUHdCn1KWw6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add components to your pipeline\n",
        "retrieval_pipeline = Pipeline()\n",
        "retrieval_pipeline.add_component(name=\"text_embedder\", instance=text_embedder)\n",
        "retrieval_pipeline.add_component(name=\"embedding_retriever\", instance=embedding_retriever)\n",
        "retrieval_pipeline.add_component(name=\"bm25_retriever\", instance=bm25_retriever)\n",
        "retrieval_pipeline.add_component(name=\"document_joiner\", instance=document_joiner)\n",
        "retrieval_pipeline.add_component(name=\"ranker\", instance=ranker)\n",
        "retrieval_pipeline.add_component(name=\"prompt_builder\", instance=prompt_builder)\n",
        "retrieval_pipeline.add_component(name=\"llm\", instance=llm)\n",
        "\n",
        "# Now, connect the components to each other\n",
        "retrieval_pipeline.connect(sender=\"text_embedder\", receiver=\"embedding_retriever\")\n",
        "retrieval_pipeline.connect(sender=\"bm25_retriever\", receiver=\"document_joiner\")\n",
        "retrieval_pipeline.connect(sender=\"embedding_retriever\", receiver=\"document_joiner\")\n",
        "retrieval_pipeline.connect(sender=\"document_joiner\", receiver=\"ranker\")\n",
        "retrieval_pipeline.connect(sender=\"ranker.documents\", receiver=\"prompt_builder.documents\")\n",
        "retrieval_pipeline.connect(sender=\"prompt_builder.prompt\", receiver=\"llm.prompt\")"
      ],
      "metadata": {
        "id": "apWHGwMJrap8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize the RAG Pipeline"
      ],
      "metadata": {
        "id": "S4AjMe75idXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_pipeline.draw(path=\"retrieval_pipeline.png\")\n",
        "Image(\"retrieval_pipeline.png\")"
      ],
      "metadata": {
        "id": "xgaMpaQaifb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "1fe21308-d9e4-49e1-e2fd-d65dac806358"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'retrieval_pipeline' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a9a3e1d92547>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mretrieval_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"retrieval_pipeline.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"retrieval_pipeline.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'retrieval_pipeline' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Serialize the RAG pipeline"
      ],
      "metadata": {
        "id": "brN1C1yJtgoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yaml_pipeline = retrieval_pipeline.dumps()\n",
        "rich.print(yaml_pipeline)"
      ],
      "metadata": {
        "id": "Uy40oKdbtgu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the RAG Pipeline"
      ],
      "metadata": {
        "id": "4qFK7QsY1Y87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_question(query: str) -> dict:\n",
        "  \"\"\"Make question using the LLM supported by RAG.\"\"\"\n",
        "  data = {\n",
        "       \"text_embedder\": {\"text\": query},\n",
        "       \"bm25_retriever\": {\"query\": query},\n",
        "       \"ranker\": {\"query\": query},\n",
        "       \"prompt_builder\": {\"query\": query}\n",
        "   }\n",
        "  result = retrieval_pipeline.run(\n",
        "       data=data,\n",
        "       debug=False,\n",
        "       include_outputs_from={\"ranker\", \"prompt_builder\"}\n",
        "   )\n",
        "  return result"
      ],
      "metadata": {
        "id": "AyMknm7PgN2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make questions using the RAG Pipeline"
      ],
      "metadata": {
        "id": "Rwy73q-SYicQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer = make_question(query=\"I would to watch a Marvel or DC superhero movie\")\n",
        "rich.print(answer)"
      ],
      "metadata": {
        "id": "9TMUJzBJ0GjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer = make_question(query=\"In what year did Juventus win its last Champions League?\")\n",
        "rich.print(answer[\"llm\"][\"replies\"][0])"
      ],
      "metadata": {
        "id": "ksFLG4Y2HRHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f9SpOI2EJURK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}